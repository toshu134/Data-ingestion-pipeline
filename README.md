# ğŸš€ Delta Lake ETL Pipeline with AI Email Summary

This project demonstrates a fully automated **ETL pipeline** using **PySpark**, **Delta Lake**, and **LlamaIndex** to generate fake data every few minutes, store it in Delta format, and email a smart summary of the data using a large language model (LLM).

---

## ğŸ” Key Features

- ğŸ” Fake data generation using `Faker`
- âš¡ Ingestion into Delta Lake with Spark
- ğŸ§  AI-powered summary using `LlamaIndex` + `Groq`
- ğŸ“© Email delivery with Gmail SMTP
- â±ï¸ Runs every 5 minutes in a loop
- ğŸ”„ Tracks Delta version before and after write

---

## ğŸ—‚ï¸ Project Structure

```bash
celebal-project/
â”œâ”€â”€ data_generation.py     # Generates fake name, address, email using Faker
â”œâ”€â”€ Ingestion_csv          # (Auto-generated) stores the being generated into a csv file 
â”œâ”€â”€ email_utils.py         # Uses LlamaIndex + Groq LLM to generate summary and send email
â”œâ”€â”€ main_pipeline.py       # Master pipeline: generates, ingests, reads, summarizes and emails
â”œâ”€â”€ spark.py               # Handles Spark session init, Delta write/read/versioning
â”œâ”€â”€ data/                  # (Auto-generated) Delta table directory with version history
â”œâ”€â”€ requirements.txt       # List of Python packages
â””â”€â”€ README.md              # Project overview and instructions
```

---
## ğŸ’¾ Data Storage Details

- **CSV (Optional):**  
  While currently not written, the fake data can easily be saved as a `.csv` using:
  ```python
  df.to_csv(f"batch_{timestamp}.csv", index=False)
  ```
  Useful for keeping a local log of batches.

- **Delta Table:**  
  Spark writes all records to a Delta table stored in the `data/` directory in **transactional parquet format**, maintaining **version history** using `DeltaTable.history()`.
---

---
## âš™ï¸ How It Works

1. `data_generation.py` creates 1000 fake records every cycle.
2. `spark.py` writes this data into a Delta Lake table and tracks its version history.
3. `main_pipeline.py` runs the full loop: generate â†’ ingest â†’ read â†’ summarize â†’ email.
4. `email_utils.py` uses **LlamaIndex** to summarize the ingested data and sends the summary + preview via email.

---

## ğŸš€ Setup Instructions

### Step 1: Install Dependencies

Make sure you have Python 3.8+ and Java 8 or 11 installed (`java -version` to check).

Then, run:

```bash
pip install -r requirements.txt
```

---

## ğŸ§ª Running the Pipeline

To start the looped pipeline:

```bash
python main_pipeline.py
```

It will:
- Generate fake user data
- Append it to a Delta table
- Track version before & after write
- Summarize new data via Groq LLM
- Email the summary every 5 minutes

---

## ğŸ” Credentials To Replace

In `email_utils.py`:
- Replace `"your_groq_api_key"` with your **Groq API key**
- Replace Gmail credentials (`server.login(...)`) with your Gmail app password. Enable 2FA and generate an app password:  
  [How to Get App Password â†’](https://support.google.com/accounts/answer/185833)

---
![image](https://github.com/user-attachments/assets/4a894589-4143-40db-9cac-deef973fc144)

![image](https://github.com/user-attachments/assets/18ac42e9-45b4-402f-8d2f-a5d8d61bdf13)


## ğŸ‘¤ Author

**Arnav Kumar Singh**  
B.Tech CSE | Data & AI Enthusiast  
ğŸ“§ arnavsingh0325@gmail.com  
ğŸŒ [LinkedIn](https://www.linkedin.com/in/arnav-kumar-singh)

---

## ğŸ›¡ License

MIT License â€“ Free to use for learning and demonstration.
